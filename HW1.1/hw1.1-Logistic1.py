#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  6 11:47:48 2021

@author: sunhaoxian
"""
#import all the packages needed
import json
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from scipy.optimize import minimize

# A class used to generate display the original data set for further use
class Data:
    #INITIALIZER
    def __init__(self, name):
        self.name = name
        self.out = {}
    
    #read in file
    def write_json(self):
        with open(self.name, "w") as write_file:
            json.dump(self.out, write_file)
            
    #generate the age, weight and is_adult data
    def partition(self):
        self.out["xlabel"]="age"
        self.out["ylabel"]="weight"
        N=250; xmin=3; xmax=100; SF=0.12
        self.x = np.linspace(xmin, xmax, N)
        y = 181.0/(1+np.exp(-(self.x-13)/4))+20
        noise = SF*(max(y)-min(y))*np.random.uniform(-1,1,size=len(self.x))
        self.yn = y + noise
        self.out["x"] = self.x.tolist()
        self.out["y"] = self.yn.tolist()
        A_or_C = [] 
        for i in range(0,len(self.x)):
            if(self.x[i]<18):
                A_or_C.append(0)
            else:
                A_or_C.append(1)
        self.out["is_adult"] = A_or_C
        
    #display the original dataset
    def plot(self):
        fig, ax = plt.subplots()
        ax.plot(self.x, self.yn, 'o', label = "weight")
        ax.legend()
        FS=18   #FONT SIZE
        plt.xlabel(self.out["xlabel"], fontsize=FS)
        plt.ylabel(self.out["ylabel"], fontsize=FS)
        plt.title("Original Dataset")
        plt.show()
        
        
#class to conduct logistic regression on dataset
class Logistic1:
    w_data = Data("weight.json")
    w_data.write_json()
    w_data.partition()
    iterations=[]
    loss_train=[]
    loss_val=[]
    ite = 0
    
    #initialize x and y and their means and standard deviations respectively
    #in this case, x is age and y is weight
    def __init__(self):
        self.x = self.w_data.out["x"]
        self.y = self.w_data.out["y"]
        self.y_mean = np.mean(self.y)
        self.y_std = np.std(self.y)
        self.x_mean = np.mean(self.x)
        self.x_std = np.std(self.x)
        
    #split the dataset into 2 groups, 20% test and 80% train
    def split(self):
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=0.20)
        
    #normalize the inputs and outputs using standard scaler
    def normalize(self):
        self.x_nor_train = [(val - self.x_mean) / self.x_std for val in self.x_train]
        self.y_nor_train = [(val - self.y_mean) / self.y_std for val in self.y_train]
        self.x_nor_test = [(val - self.x_mean) / self.x_std for val in self.x_test]
        self.y_nor_test = [(val - self.y_mean) / self.y_std for val in self.y_test]
        self.x_nor = [(val - self.x_mean) / self.x_std for val in self.x]
        
    #This is the function to generate the predicted y with inputs x and p by logistic regression
    def modelFunc(self, x, p):
        return [p[0]+p[1]*(1.0/(1.0+np.exp(-(i-p[2])/(p[3]+0.00001)))) for i in x]
    
    #The loss function to record and return the mean squard error between the true values 
    #and predicted values. Further, by applying minimize to this function, we can find
    #the best p that can minize the mean square error and store it as our linear 
    #regression function. 
    def loss(self, p):
        y_pred = self.modelFunc(self.x_nor_train, p)
        train_loss = mean_squared_error(self.y_nor_train, y_pred)
        self.loss_train.append(train_loss)
        y_test_pred = self.modelFunc(self.x_nor_test, p)
        test_loss = mean_squared_error(self.y_nor_test, y_test_pred)
        self.loss_val.append(test_loss)
        self.ite += 1
        self.iterations.append(self.ite)
        return train_loss
    
    #unnormalize the predicted y generated by the modelFunc above, to compare these
    #results with the true values
    def unnormalize(self, y_pred):
        return [self.y_std * i + self.y_mean for i in y_pred]
    
    #display the loss change as the number of iterations increases
    def displayLoss(self):
        fig, ax = plt.subplots()
        ax.plot(self.iterations, self.loss_train, '--', label='train loss')
        ax.plot(self.iterations, self.loss_val, '-', label='test loss')
        ax.legend()
        FS=18   #FONT SIZE
        plt.xlabel('Number of Iterations', fontsize=FS)
        plt.ylabel('Loss', fontsize=FS)
        plt.title("Loss vs Iterations")
        plt.show()
        
#Main Function
if __name__ == "__main__":
    data = Data("weight.json")
    data.write_json()
    data.partition()
    data.plot()
    
    #initialize the logistic regression
    log = Logistic1()
    log.split()
    log.normalize()
    NFIT = 4
    #RANDOM INITIAL GUESS FOR FITTING PARAMETERS
    po=np.random.uniform(0.5,1.,size=NFIT)
    #obtain the best coefficients
    res = minimize(log.loss, po, method='BFGS', tol=1e-15)
    popt=res.x
    print("OPTIMAL PARAM:",popt)
    #display the loss change
    log.displayLoss()
    
    #build the model on the whole dataset
    x = log.x_nor
    y_pred_norm = log.modelFunc(x, popt)
    y_pred = log.unnormalize(y_pred_norm)
    
    #show the plots
    fig, ax = plt.subplots()
    ax.plot(log.x_train, log.y_train, 'g*', label = "train set")
    ax.plot(log.x_test, log.y_test, 'rx', label = "test set")
    ax.plot(log.x, y_pred, 'k', label='logistic regression')
    ax.legend()
    FS=18   #FONT SIZE
    plt.xlabel("age", fontsize=FS)
    plt.ylabel("weight", fontsize=FS)
    plt.title("Logistic Regression")
    plt.show()
